#!/bin/bash
#
# Small wrapper script to push all docs from a nutch crawl into search
#
#

cygwin=false
case "`uname`" in
CYGWIN*) cygwin=true;;
esac

# resolve links - $0 may be a softlink
THIS="$0"
while [ -h "$THIS" ]; do
  ls=`ls -ld "$THIS"`
  link=`expr "$ls" : '.*-> \(.*\)$'`
  if expr "$link" : '.*/.*' > /dev/null; then
    THIS="$link"
  else
    THIS=`dirname "$THIS"`/"$link"
  fi
done


# Find out where we were called from, this is our home.
CRAWL_DIR="`dirname "$THIS"`"
CRAWL_HOME="`cd "$CRAWL_DIR/.." ; pwd`"

# Command Parameters:
# Usage: Indexer <crawldb> [-linkdb <linkdb>] [-params k1=v1&k2=v2...] (<segment> ... | -dir <segments>) [-noCommit] [-deleteGone] [-filter] [-normalize]

#Index all segments
"$CRAWL_HOME/bin/runnutch" index "$CRAWL_HOME"/crawl_data/crawldb -linkdb "$CRAWL_HOME"/crawl_data/linkdb -dir "$CRAWL_HOME"/crawl_data/segments 

# Clean collection of dupes, etc.
"$CRAWL_HOME/bin/runnutch" clean "$CRAWL_HOME"/crawl_data/crawldb 

